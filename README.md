## 翻译论文 
## Very Deep Convolutional Networks For Large-Scale Image Recognition
### Abstract
本文，我们研究在大规模图像识别任务中，卷积神经网络的深度对准确率的影响。我们用一个由3X3卷积核组成的网络框架来进行实验。结果表明通过把网络从16层加到19层weight layers，准确率会有很大改善。这一发现使得我们在2014年的ImageNet Challenge比赛的定位和分类任务中分别获得了第一名和第二名的成绩。而且我们的研究成果也适用于其他数据集，其模型也是目前表现最好的。我们已经把两个表现最好的卷积神经网络模型公之于众，从而能够促进机器视觉的发展。

###Itroduction
最近，在大规模图像识别和视频识别中，卷积神经网络发挥了重大作用。这主要归功于大的图像数据库，比如ImageNet，以及计算机性能的提升，比如GPU。尤其是，ImageNet Large-Scale Visual Recognition Challenge(ILSVRC)挑战赛对深度视觉识别框架的发展起到了很大的推动作用。从high-dimentional shallow feature encodings到深度卷积神经网络，ILSVRC挑战赛孵化出了好几代大规模图像分类系统(large-scale image classification systems)。

随着卷积神经网络在计算机视觉领域的商业价值越来越高，很多人试图改进最初Krizhevsky et al,(2012)创建的框架,从而获得更高的准确率。例如，在ILSVRC-2013挑战赛中，表现最好的提交在模型的第一层用更小的窗口和更小的步幅。还有人用不同同一图像的不同尺度来训练网络和测试网络(Sermanet et al.,2014;Howard,2014)。本文，我们着重探索卷积神经网络另一个重要的特征---深度。我们保持网络其他超参数不变，不断的增加卷积层数，并且每一层的卷积核都是3X3大小。

结果，我们得到了准确度更好的神经网络框架，不仅在比赛中取得了最好的成绩，而且该网络框架还适用于其他图像数据集，并且表现很好。

### 2 ConvNet参数设置

#### 2.1 框架Architecture
网络的输入是224X224 三通道彩色RGB图像。预处理只有一个步骤，就是计算所有训练集图像的平均像素值，然后用每个图像的每个像素减去该平均像素值。然后图像传入一系列的卷积层，用3X3这种最小的卷积核获取图像中的纹理信息。有时候也使用1X1的卷积核，这种卷积核用来做线性变换。卷积步长是1 pixel。在对图像卷积之间要进行边缘填充padding，使得卷积之前和卷积之后的图像分辨率不变。我们还添加了五个最大池化层，分布在卷积层之后。最大池化层用的是2X2的窗口，步长是2。

在卷积层之后是三个全连接层Fully-Connected(FC) layers：前两层分别有4096个神经元，第三层有1000个神经元，因为数据集有1000个分类,最好一层是soft-max层。不同网络的全连接层设置都是一样的。所有的隐藏层都是用ReLU非线性矫正函数。还有，我们的所有网络（除了其中一个）都不包含Local Response Normalisation(LRN)层(Krizhevsky et aal.,2012)。因为这种归一化层在ILSVRC数据集上并不能改善性能，反而增加内存消耗和计算时间。如果你要用这种归一化层的话，它的参数设置请参考(Krizhevsky et al.,2012)。

#### 2.2 参数设置Configurations
本文的网络参数设置列在了表一中。共有A~E个网络。每个网络的设计参数都是相同的，只是深度不同。从网络A包含的11层（包含8个卷积层和3个全连接层）到网络E包含的19层（包含16个卷积层，3个全连接层）。每个卷积层的通道数不同。网络第一个卷积层是64通道，每经过一个最大池化层通道数就增大一倍，直到到达512。

在表2，我们展示了不同深度的网络包含的参数数量。我么发现，虽然网络深度相差很大，但是要训练的参数数量却相差不大。
[表1表2](https://github.com/May522/translation-vgg/blob/master/%E6%8D%95%E8%8E%B71.JPG)
#### 2.3 讨论
我们的网络参数设置和目前表现好的网络参数设置不同。他们都是用11 * 11或者7 * 7的卷积核，我们只用最小的3 * 3的卷积核。两个3 * 3的卷积核相当于一个5 * 5的卷积核。三个3 * 3的卷积核相当于一个7 * 7的卷积核。那么我们为什么不用一个7 * 7的卷积核而选择三个3 * 3的卷积核。原因是，第一，我们在每个3 * 3的卷积核后面添加了非线性校正函数，使得边界函数更具灵活性。第二，假设用三个3 * 3的卷积核处理一个含有C个通道的卷积层，那么要训练的参数是3(3^2 * C^2)=27C^2个。如果是用7 * 7的卷积核来处理，需要训练的参数是7^2 * C^2=49C^2个。

其中1 * 1的卷积层是为了增加决策函数的非线性。本文中的1 * 1卷积核是一种线性映射，另外用校正函数来添加非线性。

先前，Ciresan te al,(2011)在他的网络中用小型的卷积核，但是他的网络不够深，并且他们用的数据集也不是ILSVRC。Goodfelloe et al,(2014)在街拍数字识别street number recognition数据集中，用的卷积神经网络包含11层，研究表明网络越深，性能越好。GoogleLeNet(Szegedy et al.,2014)在ILSVRC-2014竞赛中表现最好，他们用的卷积神经网络包含22层，并且用的卷积核很小，除了3 * 3的，还用1 * 1的和5 * 5的。他们的网络结构比我们的更复杂。相比较下，我们的网络在分类准确度上比他们的网络表现更好。





